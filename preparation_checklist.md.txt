# Genome Analysis Pipeline - Preparation Checklist

## ğŸ“‹ Action Items for This Week

Use this checklist to prepare for next week's pipeline development session.

---

## ğŸ—„ï¸ Priority 1: Essential Database Downloads

**Estimated time: 4-8 hours depending on connection speed**
**Estimated storage needed: ~200 GB**

| # | Database | Size | Command | Status |
|---|----------|------|---------|--------|
| 1 | **Kraken2 Standard** | ~100 GB | `kraken2-build --standard --threads 16 --db /databases/kraken2/standard` | â˜ |
| 2 | **Bakta** | ~30 GB | `bakta_db download --output /databases/bakta --type full` | â˜ |
| 3 | **GTDB-Tk** | ~85 GB | See download script | â˜ |
| 4 | **CheckV** | ~2 GB | `checkv download_database /databases/checkv` | â˜ |
| 5 | **geNomad** | ~3 GB | `genomad download-database /databases/genomad` | â˜ |
| 6 | **CARD** | ~500 MB | Download from card.mcmaster.ca | â˜ |
| 7 | **AMRFinderPlus** | ~500 MB | `amrfinder_update -d /databases/amrfinder` | â˜ |
| 8 | **VFDB** | ~100 MB | Download from mgc.ac.cn + makeblastdb | â˜ |
| 9 | **PubMLST** | ~1 GB | `mlst --update` | â˜ |
| 10 | **Pharokka** | ~8 GB | `install_databases.py -o /databases/pharokka` | â˜ |

### Quick Start
```bash
# Run the provided download script
chmod +x preparation_scripts/download_databases.sh
./preparation_scripts/download_databases.sh /path/to/databases
```

---

## ğŸ§ª Priority 2: Prepare Test Data

Please prepare the following test datasets. We'll use these to validate the pipeline during development.

### Required Test Cases

| Test Case | What to Provide | Purpose |
|-----------|-----------------|---------|
| **TC1: Known bacterium** | Illumina reads (R1+R2 FASTQ) from a well-characterized strain | Validate full bacterial pipeline |
| **TC2: Known virus** | Reads from a known virus | Validate viral pipeline |
| **TC3: Nanopore data** | Nanopore FASTQ from any organism | Validate long-read path |
| **TC4: Hybrid data** | Both Illumina + Nanopore from same sample | Validate hybrid assembly |

### For Each Test Case, Please Provide:

1. **Raw reads** (FASTQ format, gzipped OK)
   - Illumina: R1 and R2 files
   - Nanopore: Single FASTQ file
   - Size: 100k-1M reads is sufficient (we can work with subsets)

2. **Ground truth document** (text/spreadsheet):
   - Species/strain name
   - Expected genome size
   - Known AMR genes (if any)
   - Known plasmids (if any)
   - Known virulence factors (if any)
   - MLST type (if known)
   - Any special features we should detect

3. **Reference genome** (if available):
   - GenBank or FASTA file
   - Annotation GFF (if available)

### Test Data Collection Form

```
Test Case ID: ________________
Sample Name: ________________
Organism: ________________ (species/strain)
Genome Size: ________________ bp
Sequencing Platform: â˜ Illumina  â˜ Nanopore  â˜ Both

Files:
- [ ] Forward reads: ________________
- [ ] Reverse reads: ________________
- [ ] Long reads: ________________
- [ ] Reference genome: ________________

Known Features:
- AMR genes: ________________
- Plasmids: ________________
- Virulence factors: ________________
- MLST: ________________
- Special features: ________________

Notes:
________________
```

---

## ğŸ³ Priority 3: Docker Images (Optional but Recommended)

If using Docker, pull and save these images while you have internet:

```bash
# Pull essential images
docker pull staphb/spades:3.15.5
docker pull staphb/flye:2.9.3
docker pull staphb/unicycler:0.5.0
docker pull staphb/quast:5.2.0
docker pull staphb/kraken2:2.1.3
docker pull staphb/prokka:1.14.6
docker pull staphb/bakta:1.9.2
docker pull staphb/mlst:2.23.0
docker pull staphb/abricate:1.0.1
docker pull ncbi/amrfinderplus:latest
docker pull oschwengers/pharokka:latest
docker pull nanoporetech/medaka:latest

# Save images for offline transfer (optional)
mkdir -p docker_images
docker save staphb/spades:3.15.5 | gzip > docker_images/spades_3.15.5.tar.gz
# ... repeat for other images
```

---

## âœ… Validation

Before next week, run the validation script to ensure everything is ready:

```bash
chmod +x preparation_scripts/validate_installation.sh
./preparation_scripts/validate_installation.sh /path/to/databases
```

Expected output: All critical checks should pass.

---

## ğŸ“ Recommended Directory Structure

Set up this structure on your server:

```
/databases/                    # All database files
â”œâ”€â”€ kraken2/
â”œâ”€â”€ gtdbtk/
â”œâ”€â”€ bakta/
â”œâ”€â”€ checkv/
â”œâ”€â”€ genomad/
â”œâ”€â”€ card/
â”œâ”€â”€ amrfinder/
â”œâ”€â”€ vfdb/
â”œâ”€â”€ pharokka/
â””â”€â”€ database_paths.yaml        # Generated by download script

/test_data/                    # Test datasets
â”œâ”€â”€ TC1_known_bacteria/
â”‚   â”œâ”€â”€ reads_R1.fastq.gz
â”‚   â”œâ”€â”€ reads_R2.fastq.gz
â”‚   â”œâ”€â”€ ground_truth.txt
â”‚   â””â”€â”€ reference.gbk
â”œâ”€â”€ TC2_known_virus/
â”œâ”€â”€ TC3_nanopore/
â””â”€â”€ TC4_hybrid/

/pipeline/                     # We'll create this together
â””â”€â”€ (empty for now)
```

---

## ğŸ’¬ Questions to Answer Before Next Week

Please think about these and let me know:

1. **Sample metadata**: What information will typically accompany samples?
   - Sample ID format?
   - Collection date?
   - Source type (clinical, environmental)?
   - Any other standard fields?

2. **Reference-guided analysis**: Will you often have a specific reference genome to compare against?

3. **Batch processing**: Need to analyze many samples at once? Comparative analysis across samples?

4. **Organism-specific modules**: Any species you work with frequently that deserve specialized analysis?
   - Common clinical pathogens?
   - Specific research organisms?

5. **Alert thresholds**: What findings should be highlighted/flagged?
   - High-risk AMR genes?
   - Specific virulence factors?
   - Completeness thresholds?

---

## ğŸ“ Communication

If you encounter issues with database downloads or have questions:
- Document any error messages
- Note which databases succeeded vs failed
- Let me know what test data you've prepared

---

## ğŸ“… Timeline Summary

| Day | Task |
|-----|------|
| Day 1-2 | Start database downloads (Kraken2, GTDB-Tk take longest) |
| Day 2-3 | Continue downloads, prepare test data |
| Day 3-4 | Prepare ground truth documentation for test data |
| Day 4-5 | Run validation script, fix any issues |
| Day 5-6 | Pull Docker images (if using), backup databases |
| Day 7 | Final verification, ready for pipeline development |

---

*Checklist created: December 2024*